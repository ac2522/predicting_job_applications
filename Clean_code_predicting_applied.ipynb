{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pycodestyle_magic extension is already loaded. To reload it, use:\n",
      "  %reload_ext pycodestyle_magic\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from math import radians, sin, cos, acos\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from itertools import zip_longest\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from operator import attrgetter\n",
    "\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pandas.io.sql as psql\n",
    "import sqlite3 as sql\n",
    "import psycopg2\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "from sklearn import preprocessing, feature_extraction, metrics, tree\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn.model_selection import KFold, cross_val_score, StratifiedKFold, train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, SGDClassifier, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_union\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report, recall_score\n",
    "from sklearn.metrics import r2_score, balanced_accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from collections import Counter\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "import eli5\n",
    "import xgboost as xgb\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K\n",
    "from keras import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.callbacks import LearningRateScheduler, Callback\n",
    "\n",
    "import lookahead\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import gensim, logging\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "plt.rc(\"font\", size=14)\n",
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "\n",
    "postgres_str = (\"...\")\n",
    "cnx = create_engine(postgres_str)\n",
    "%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Which invited candidates applied?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(cnx, shortlist=False):  # returns columns job_profile_d and whether applied and shortlisted\n",
    "    Status_trans = pd.read_sql_query('''SELECT * FROM ops_StatusTransition;''', cnx)\n",
    "    Stat = Status_trans[Status_trans['to_status'] == 'invited-apply']['job_profile_id']\n",
    "    Status_trans = Status_trans[Status_trans['job_profile_id'].isin(Stat.tolist())]\n",
    "    invited_ = Status_trans[Status_trans['to_status'] == 'invited-apply'].drop_duplicates(subset='job_profile_id')\n",
    "    applied_ = Status_trans[Status_trans['to_status'] == 'applied'].drop_duplicates(subset='job_profile_id')\n",
    "    invited_['applied'] = invited_['job_profile_id'].isin(applied_['job_profile_id']).astype(int)\n",
    "    if shortlist is True:\n",
    "        shortlisted_ = Status_trans[Status_trans['to_status'] == 'added-to-shortlist']\n",
    "        shortlisted_.drop_duplicates(subset='job_profile_id', inplace=True)\n",
    "        invited_['shortlisted'] = invited_['job_profile_id'].isin(shortlisted_['job_profile_id']).astype(int)\n",
    "        return invited_[['job_profile_id', 'created', 'applied', 'shortlisted']]\n",
    "    return invited_[['job_profile_id', 'created', 'applied']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How was a candidate sourced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_source(value):  # Splits sources\n",
    "    sources = value['source'].split(\" | \")\n",
    "    value['source2'] = sources[1]\n",
    "    value['source'] = sources[0]\n",
    "    return(value)\n",
    "\n",
    "\n",
    "def internal_s(value):  # Numbers internal sourced candidates\n",
    "    if value.source == 'Jump Search':\n",
    "        return 1\n",
    "    elif value.source == 'ShortList':\n",
    "        return 2\n",
    "    elif value.source == 'Talent Pool':\n",
    "        return 3\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def external_s(value):  # Numbers external sourced candidates\n",
    "    if value.source == 'Reed Search' or value.source == 'Reed':\n",
    "        return 1\n",
    "    elif value.source == 'SecsintheCity':\n",
    "        return 2\n",
    "    elif value.source == 'Totally Legal':\n",
    "        return 3\n",
    "    elif value.source == 'Jump Direct':\n",
    "        return 4\n",
    "    elif value.source == 'GAAPWeb':\n",
    "        return 5\n",
    "    elif value.source == 'TotalJobs':\n",
    "        return 6\n",
    "    elif value.source == 'Jobsite':\n",
    "        return 7\n",
    "    elif value.source == 'External Job Post':\n",
    "        return 8\n",
    "    elif value.source == 'Outreach':\n",
    "        return 8\n",
    "    elif value.source == 'Indeed':\n",
    "        return 9\n",
    "    elif value.source == 'SimplyJobs':\n",
    "        return 10\n",
    "    elif value.source == 'Scraper':\n",
    "        if value.source2 == 'Sitc':\n",
    "            return 2\n",
    "        elif value.source2 == 'Totallylegal':\n",
    "            return 3\n",
    "        else:\n",
    "            return 5\n",
    "    elif value.source == 'Upload CV':\n",
    "        if value.source2 == 'Totaljobs':\n",
    "            return 6\n",
    "        elif value.source2 == 'Totally_Legal':\n",
    "            return 3\n",
    "        elif value.source2 == 'Jobsite':\n",
    "            return 7\n",
    "        elif value.source2 == 'Indeed':\n",
    "            return 9\n",
    "        elif value.source2 == 'Gaapweb':\n",
    "            return 5\n",
    "        elif value.source2 == 'Secsinthecity':\n",
    "            return 2\n",
    "        elif value.source2 == 'Cvlibrary':\n",
    "            return 11\n",
    "        elif value.source2 == 'Simplyjobs':\n",
    "            return 10\n",
    "        elif value.source2 == 'Reed':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    elif value.source == 'Custom':\n",
    "        if value.source2 == 'Indeed Messaging Tracking Link' or value.source2 == 'indeed':\n",
    "            return 10\n",
    "        elif value.source2 == 'TotalJobs':\n",
    "            return 6\n",
    "        else:\n",
    "            return 8\n",
    "    elif value.source == 'Scraper':\n",
    "        if value.source2 == 'Sitc':\n",
    "            return 2\n",
    "        elif value.source2 == 'Totallylegal':\n",
    "            return 3\n",
    "        else:\n",
    "            return 5\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_sourcing():\n",
    "    with open('sourc_0.json') as f:\n",
    "        data0 = json.load(f)\n",
    "    with open('new_apps.json') as f:\n",
    "        data05 = json.load(f)\n",
    "    with open('sourc_1.json') as f:\n",
    "        data1 = json.load(f)\n",
    "    with open('sourc_2.json') as f:\n",
    "        data2 = json.load(f)\n",
    "    with open('sourc_3.json') as f:\n",
    "        data3 = json.load(f)\n",
    "    with open('sourc_4.json') as f:\n",
    "        data4 = json.load(f)\n",
    "    with open('sourc_5.json') as f:\n",
    "        data5 = json.load(f)\n",
    "    with open('sourc_6.json') as f:\n",
    "        data6 = json.load(f)\n",
    "    with open('sourc_7.json') as f:\n",
    "        data7 = json.load(f)\n",
    "    with open('sourc__0.json') as f:\n",
    "        data8 = json.load(f)\n",
    "    with open('sourc__1.json') as f:\n",
    "        data9 = json.load(f)\n",
    "    with open('sourc__2.json') as f:\n",
    "        data10 = json.load(f)\n",
    "    with open('sourc__3.json') as f:\n",
    "        data11 = json.load(f)\n",
    "    with open('sourc__4.json') as f:\n",
    "        data12 = json.load(f)\n",
    "    with open('sourc__5.json') as f:\n",
    "        data13 = json.load(f)\n",
    "    z = {**data0, **data05, **data1, **data2, **data3, **data4, **data5, **data6,\n",
    "         **data7, **data8, **data9, **data10, **data11, **data12, **data13}\n",
    "    data_ = pd.Series(z).to_frame()\n",
    "    data_.columns = ['source']\n",
    "    data_ = data_.dropna(subset=['source'])\n",
    "    data_['job_profile_id'] = data_.index\n",
    "    data_ = data_.set_index(pd.Series(range(len(data_))))\n",
    "    data_['source2'] = \"\"\n",
    "    temp_var = data_[data_['source'].str.contains(' | ', regex=False)].apply(split_source, axis=1)\n",
    "    data_[data_['source'].str.contains(' | ', regex=False)] = temp_var\n",
    "    data_['s_external_overall'] = data_['s_internal_overall'] = 0\n",
    "    data_ = data_.assign(s_internal_overall=data_.apply(internal_s, axis=1))\n",
    "    data_ = data_.assign(s_external_overall=data_.apply(external_s, axis=1))\n",
    "    data_ = pd.get_dummies(data_, prefix=['source_i'], columns=['s_internal_overall'])\n",
    "    data_ = pd.get_dummies(data_, prefix=['source_e'], columns=['s_external_overall'])\n",
    "    return data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_application_rate(data):\n",
    "    return data['applied'].mean() * 100\n",
    "\n",
    "\n",
    "def compute_application_rate_per(data, company=True, job=False):\n",
    "    merged_data = pd.merge(data, JobProfile, how=\"inner\", left_on=\"job_profile_id\",\n",
    "                           right_on=\"job_profile_id\", suffixes=(\"_\", \"_Job\"))\n",
    "    merged_data = pd.merge(merged_data, Job_Post, how=\"inner\", left_on=\"job_post_id\",\n",
    "                           right_on=\"job_post_id\", suffixes=(\"_\", \"_Job\"))\n",
    "    if job is True:\n",
    "        if company is True:\n",
    "            print(merged_data.groupby('company_id').applied.agg(['mean', 'count']))\n",
    "        return merged_data.groupby('job_post_id').applied.agg(['mean', 'count'])\n",
    "    elif company is True:\n",
    "        return merged_data.groupby('company_id').applied.agg(['mean', 'count'])\n",
    "    else:\n",
    "        return 'ERROR: Neither company nor job was selected'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance and time from postcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def miles_to_metres(miles):\n",
    "    return miles * MILES_TO_METRES\n",
    "\n",
    "\n",
    "MILES_TO_METRES = 1.609344\n",
    "inner_radius = {'East Midlands':  miles_to_metres(12), 'East of England': miles_to_metres(20),\n",
    "                'London':  miles_to_metres(8), 'South East': miles_to_metres(12), 'South West': miles_to_metres(8),\n",
    "                'West Midlands': miles_to_metres(8), 'Other': miles_to_metres(18)}\n",
    "outer_radius = {'East Midlands':  miles_to_metres(40), 'East of England': miles_to_metres(25),\n",
    "                'London':  miles_to_metres(15), 'South East':  miles_to_metres(29), 'South West':  miles_to_metres(20),\n",
    "                'West Midlands': miles_to_metres(17), 'Other': miles_to_metres(24)}\n",
    "\n",
    "\n",
    "def within_commute(distance, region='Other', commute_time=60):\n",
    "    if commute_time != commute_time:\n",
    "        commute_time = 60  # Basic distance to time conversion, based on regions\n",
    "    if region not in inner_radius:\n",
    "        region = 'Other'\n",
    "    radii = inner_radius\n",
    "    if commute_time < 40:\n",
    "        flexibility = 1.25\n",
    "    elif commute_time < 60:\n",
    "        flexibility = 1.5\n",
    "    elif commute_time < 90:\n",
    "        flexibility = 2\n",
    "    else:\n",
    "        radii = outer_radius\n",
    "        flexibility = 1\n",
    "    within = (radii.get(region) * flexibility) - distance\n",
    "    if within < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def grouper(n, iterable, fillvalue=None):\n",
    "    \"grouper(3, 'ABCDEFG', 'x') --> ABC DEF Gxx\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return zip_longest(fillvalue=fillvalue, *args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required for main script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preferrered_role_match(dataFrame):\n",
    "    if dataFrame['preferred_role_types'] is None:\n",
    "        return None\n",
    "    preferred = dataFrame['preferred_role_types']\n",
    "    job_post_id = dataFrame['job_post_id']\n",
    "    jj = JobTalentPool[JobTalentPool['job_id'] == job_post_id]\n",
    "    intersect = len(set(jj['_role_type_id']).intersection(set(preferred)))\n",
    "    return intersect\n",
    "\n",
    "\n",
    "def intersect_(dataFrame):\n",
    "    talent_pools_j = JobTalentPool[JobTalentPool['job_id'] == dataFrame.job_post_id]\n",
    "    talent_pools_c = CandidatesTalentPool[CandidatesTalentPool['candidate_id'] == dataFrame.candidate_id]\n",
    "    intersect = len(set(talent_pools_j['_role_type_id']).intersection(set(talent_pools_c['_role_type_id'])))\n",
    "    return intersect\n",
    "\n",
    "\n",
    "def intersect_seniority_(dataFrame):  # Checks the intersection length talent pools factoring in seniority\n",
    "    talent_pools_j = JobTalentPool[JobTalentPool['job_id'] == dataFrame.job_post_id]\n",
    "    talent_pools_c = CandidatesTalentPool[CandidatesTalentPool['candidate_id'] == dataFrame.candidate_id]\n",
    "    intersect_seniority = 0\n",
    "    for talent in set(talent_pools_j['_role_type_id']).intersection(set(talent_pools_c['_role_type_id'])):\n",
    "        j_sen = list(talent_pools_j[talent_pools_j['_role_type_id'] == talent]['seniority'])\n",
    "        c_sen = list(talent_pools_c[talent_pools_c['_role_type_id'] == talent]['seniority'])[0]\n",
    "        if c_sen in j_sen:\n",
    "            intersect_seniority += 1\n",
    "        else:\n",
    "            c_qual = list(cc[cc['_role_type_id'] == talent]['seniority_qualifier'])[0]\n",
    "            if c_qual == 'certain':\n",
    "                continue\n",
    "            elif len(j_sen) >= 1:\n",
    "                intersect_seniority += 1\n",
    "            elif c_sen == 'junior' or j_sen == 'senior':\n",
    "                intersect_seniority += 1\n",
    "    return intersect_seniority\n",
    "\n",
    "\n",
    "def basically_tinder(values):  # Similar to intersect, but on an individual roletype basis\n",
    "    talent_pools_j = JobTalentPool[JobTalentPool.job_id == values.job_post_id]['_role_type_id']\n",
    "    talent_pools_c = CandidatesTalentPool[CandidatesTalentPool.candidate_id == values.candidate_id]['_role_type_id']\n",
    "    intersect = set(talent_pools_j).intersection(set(talent_pools_c))\n",
    "    for match_num in intersect:\n",
    "        values[\"match_\" + str(match_num)] = 1\n",
    "    return values\n",
    "\n",
    "\n",
    "def yearly_salary_to_unit(Job_Post):  # converts hourly and daily salaries to yearly\n",
    "    salary = Job_Post['maximum_salary']\n",
    "    unit = Job_Post['salary_unit']\n",
    "    if unit == 'annually':\n",
    "        return salary\n",
    "    if unit == 'daily':\n",
    "        return (salary * 260)\n",
    "    if unit == 'hourly':\n",
    "        return (salary * 2080)\n",
    "\n",
    "\n",
    "def contract_employment(value):  # Do contract types match\n",
    "    if int(value.contract_work == 1) ^ int(value.permanent_work == 1):\n",
    "        if value.permanent_work == 1 and value.contract_type == 'temp':\n",
    "            value.contract_no_match = 1\n",
    "        if value.contract_work == 1 and value.contract_type == 'permanent':\n",
    "            value.contract_no_match = 1\n",
    "    if int(value.full_time_work == 1) ^ int(value.part_time_work == 1):\n",
    "        if value.employment_type is None:\n",
    "            value.employment_type = 'full-time'\n",
    "        if value.full_time_work == 1 and value.employment_type == 'part-time':\n",
    "            value.employment_no_match = 1\n",
    "        if value.part_time_work == 1 and value.employment_type == 'full-time':\n",
    "            value.employment_no_match = 1\n",
    "    return value\n",
    "\n",
    "\n",
    "def within_salary(values):  # Are Salaries compatible\n",
    "    if values.minimum_wage >= values.maximum_salary:\n",
    "        values.within_income = 1\n",
    "    return values\n",
    "\n",
    "\n",
    "def descriptor_max(value):  # Which is newest experience\n",
    "    values = value[1].dropna(subset=['description'])\n",
    "    if len(values) < 1:\n",
    "        return None,None\n",
    "    if all([values.start_year.isna().all(), values.end_year.isna().all()]):\n",
    "        return values.iloc[0].description, value[0]\n",
    "    elif any([values.start_year.max() >= values.end_year.max(), values.end_year.isna().all()]):\n",
    "        return values.loc[values.start_year.idxmax(), 'description'], value[0]\n",
    "    else:\n",
    "        return values.loc[values.end_year.idxmax(), 'description'], value[0]\n",
    "\n",
    "\n",
    "def descriptor_top3(value):  # Which is newest 3 experiences\n",
    "    values = value[1].copy()\n",
    "    for iter in range(3):\n",
    "        if all([values.start_year.isna().all(), values.end_year.isna().all()]):\n",
    "            return values[3-iter:].index.to_list()\n",
    "        elif any([values.start_year.max() >= values.end_year.max(), values.end_year.isna().all()]):\n",
    "            inds = values.start_year.idxmax()\n",
    "            values.drop([inds], inplace=True)\n",
    "        else:\n",
    "            values.drop([values.end_year.idxmax()], inplace=True)\n",
    "    return values.index.to_list()\n",
    "\n",
    "\n",
    "def True_on_going(values, True_ongoing):  # Is last job ongoing\n",
    "    if values.candidate_id in True_ongoing:\n",
    "        values.is_last_job_ongoing = 1\n",
    "        return values\n",
    "    else:\n",
    "        return values\n",
    "\n",
    "\n",
    "def index_unable(value):  # changes a few things\n",
    "    if len(value) == 1:\n",
    "        return value\n",
    "    value['seq_no'] += range(len(value))  # how many emails came before plus one\n",
    "    value['been_sent'] = 1  # This is not the first email\n",
    "    value.head(1)['been_sent'] = 0  # This is the first\n",
    "    iteration = 1\n",
    "    for ind in value.index[1:]:\n",
    "        value.loc[ind, 'invites_applied'] = value.iloc[:iteration].applied.sum()\n",
    "        if value['past_shortlist']:\n",
    "            if value.iloc[:iteration].shortlisted.sum() > 0:\n",
    "                value.loc[ind, 'past_shortlist'] = 1\n",
    "        value.loc[ind, 'mean_of_invites_applied'] = value.head(value['seq_no'][ind]-1)['applied'].mean()\n",
    "        #  mean of past application rate\n",
    "        best_estimate = 0\n",
    "        ind_pre = value.index[iteration-1:iteration][0]\n",
    "        if value.loc[ind_pre, 'applied'] == 0:\n",
    "            value.loc[ind, 'dry_spell'] = 1 + value.loc[ind_pre, 'dry_spell']\n",
    "        else:  # Dry spell, how many invites since last application\n",
    "            value.loc[ind, 'dry_spell'] = 0\n",
    "        # Looks at similarity of past invites and whether they were applied to\n",
    "        for ind2 in value.index[:iteration]:\n",
    "            applied_ = -1\n",
    "            estimate = confidential_salary = 0\n",
    "            if value.loc[ind2, 'applied'] == 1: \n",
    "                applied_ = 1\n",
    "            if value.loc[ind2, 'employment_type'] == value.loc[ind, 'employment_type']:\n",
    "                estimate += 1\n",
    "                value.loc[ind, 'similarity_emp'] += applied_\n",
    "            if value.loc[ind2, 'contract_type'] == value.loc[ind, 'contract_type']:\n",
    "                estimate += 0.6\n",
    "                value.loc[ind, 'similarity_con'] += applied_\n",
    "            if value.loc[ind2, 'company_id'] == value.loc[ind, 'company_id']:\n",
    "                estimate += 0.6\n",
    "                value.loc[ind, 'similarity_com'] += applied_\n",
    "            if value.loc[ind2, 'role_id'] == value.loc[ind, 'role_id']:\n",
    "                estimate += 1\n",
    "                value.loc[ind, 'similarity_role'] += applied_\n",
    "            if value.loc[ind2, 'job_title'] == value.loc[ind, 'job_title']:\n",
    "                estimate += 1\n",
    "                value.loc[ind, 'similarity_title'] += applied_\n",
    "            if value.loc[ind2, 'sector_id'] == value.loc[ind, 'sector_id']:\n",
    "                estimate += 0.6\n",
    "                value.loc[ind, 'similarity_sect'] += applied_\n",
    "            if value.loc[ind2, 'post_seniority_level'] == value.loc[ind, 'post_seniority_level']:\n",
    "                estimate += 1\n",
    "                value.loc[ind, 'similarity_level'] += applied_\n",
    "            if value.loc[ind2, 'asap_start'] == value.loc[ind, 'asap_start']:\n",
    "                estimate += 0.2\n",
    "                value.loc[ind, 'similarity_asap'] += applied_\n",
    "            if value.loc[ind2, 'confidential_salary'] != value.loc[ind, 'confidential_salary']:\n",
    "                confidential_salary = 1\n",
    "            elif value.loc[ind2, 'confidential_salary'] == value.loc[ind, 'confidential_salary']:\n",
    "                estimate += 1\n",
    "                value.loc[ind, 'similarity_confid'] += applied_\n",
    "            elif value.loc[ind, 'maximum_salary'] >= (value.loc[ind2, 'maximum_salary'] - 3000):\n",
    "                estimate += 1\n",
    "                value.loc[ind, 'similarity_sal'] += applied_\n",
    "            if estimate != 0:\n",
    "                estimate = estimate/(7-confidential_salary) * -1\n",
    "                estimate = estimate * (1 - value.loc[ind2, 'applied'] * 2)\n",
    "            if abs(estimate) > abs(best_estimate):\n",
    "                best_estimate = estimate\n",
    "        value.loc[ind, 'similarity_applied'] = best_estimate\n",
    "        iteration += 1\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarities between experience to job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_power_similarity(learning_data):\n",
    "    descriptions = pd.DataFrame(columns=['candidate_id', 'description'])\n",
    "    for a in CandidateEs.groupby(['candidate_id']):\n",
    "        desc,cand_key = descriptor_max(a)\n",
    "        descriptions = descriptions.append({'candidate_id' : cand_key , 'description' : desc}, ignore_index=True)\n",
    "\n",
    "    descriptions.dropna(inplace=True)\n",
    "    descriptions = descriptions.set_index('candidate_id')\n",
    "\n",
    "    descriptions2 = pd.merge(JobProfile[['job_profile_id','candidate_id']], descriptions, how='inner', left_on='candidate_id',right_on='candidate_id')\n",
    "    descriptions2 = pd.merge(learning_data[['job_profile_id','applied_x','seq_no']], descriptions2, how='inner', left_on='job_profile_id',right_on='job_profile_id')\n",
    "    descriptions3 = descriptions2[descriptions2.seq_no==1]\n",
    "    descriptions4 = descriptions3[descriptions3.description.str.len()>8]\n",
    "\n",
    "\n",
    "    embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\")\n",
    "    JobPosts = tuple(JobProfile.job_post_id.unique())\n",
    "    Job_Post = pd.read_sql_query('''SELECT id, description FROM customer_jobpost WHERE id IN %s''' % (str(JobPosts)), cnx)\n",
    "    Job_Post = Job_Post.rename(columns={'id':'job_post_id'})\n",
    "    use_on_use = list(descriptions2.description)\n",
    "    use_on_use2 = list(Job_Post.description)\n",
    "\n",
    "    tfi = TfidfVectorizer(ngram_range=(1, 4), stop_words='english')\n",
    "    X = tfi.fit_transform(descriptions4.description)\n",
    "\n",
    "    temp = np.zeros(shape=(len(descriptions2.description),512))\n",
    "    temp2 = np.zeros(shape=(X.shape[0],512))\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "    with tf.Session() as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        message_embeddings = session.run(embed(use_on_use2))\n",
    "        for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
    "            temp2[i,:] = message_embedding\n",
    "    with tf.Session() as session2:\n",
    "        session2.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        message_embeddings = session2.run(embed(use_on_use))\n",
    "        for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
    "            temp[i,:] = message_embedding\n",
    "    temp_=temp.copy()\n",
    "    temp2_=temp2.copy()\n",
    "    descriptions5 = pd.merge(descriptions2,JobProfile[['job_profile_id','job_post_id']], how='inner', left_on='job_profile_id',right_on='job_profile_id')\n",
    "    temp_3 = np.zeros(temp_.shape)\n",
    "\n",
    "    for i in range(len(descriptions5)):\n",
    "        idx = Job_Post[Job_Post.job_post_id==495].index[0]\n",
    "        temp3_[i] = temp2_[idx]\n",
    "    score = np.clip(np.sum(np.multiply(temp_3, temp_), axis=1), a_min=-1.0,a_max=1.0)\n",
    "    sim_score = 1 - np.arccos(score)\n",
    "    descriptions5[\"U_S_E\"] = sim_score #Used instead of session run_sts_benchmark\n",
    "    descriptions5[\"U_S_E_ab\"] = abs(descriptions5[\"U_S_E\"])\n",
    "    \n",
    "    learning_data = pd.merge(learning_data, descriptions5[['job_profile_id','U_S_E']], how='inner', left_on='job_profile_id',right_on='job_profile_id')\n",
    "    return learning_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words: experience position vs job title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2):\n",
    "    temp = set(lst2) \n",
    "    lst3 = [value for value in lst1 if value in temp] \n",
    "    return len(lst3)\n",
    "\n",
    "\n",
    "def title_intersection(learning_data, CandidateEs):\n",
    "    JobProfile_limited = JobProfile[JobProfile.job_profile_id.isin(learning_data.job_profile_id.to_list())]\n",
    "    Job_Post_limited = Job_Post[Job_Post.job_post_id.isin(JobProfile_limited.job_post_id.to_list())]\n",
    "    Job_Post_limited = Job_Post[Job_Post.job_post_id.isin(JobProfile_limited.job_post_id.to_list())]\n",
    "  \n",
    "    list_keep=[]\n",
    "    RecentCandidateEs = CandidateEs.dropna(subset=['description'])\n",
    "    descriptions = RecentCandidateEs[['candidate_id','position','description', 'experience_id']]\n",
    "    for a in RecentCandidateEs.groupby(['candidate_id']):\n",
    "        if len(a[1]) > 3:\n",
    "            ind = descriptor_top3(a)\n",
    "            list_keep.extend(ind)\n",
    "\n",
    "    descriptions.drop(list_keep, inplace=True)\n",
    "\n",
    "    descriptions2 = descriptions.copy()\n",
    "    descriptions2.dropna(subset = ['position'], inplace=True)\n",
    "    descriptions2.position = descriptions2.position.str.lower()\n",
    "    descriptions2.position = descriptions2.position.str.findall(r'[^\\s!,./()-?&%\":;0-9]+')\n",
    "    descriptions2.position = [w for w in descriptions2.position if not w in ['and', 'to', 'this', 'the']]\n",
    "\n",
    "    post_to_cv = pd.merge(descriptions2, JobProfile, how=\"inner\", left_on=\"candidate_id\", right_on=\"candidate_id\")\n",
    "    post_to_cv = pd.merge(post_to_cv, Job_Post, how=\"inner\", left_on=\"job_post_id\", right_on=\"job_post_id\")\n",
    "    post_to_cv[\"title_intersect\"] = post_to_cv[\"descript_intersect\"] = 0\n",
    "\n",
    "    post_to_cv.dropna(subset = ['job_title'], inplace=True)\n",
    "    post_to_cv.job_title = post_to_cv.job_title.str.lower()\n",
    "    post_to_cv.job_title = post_to_cv.job_title.str.findall(r'[^\\s!,./()-?&%\":;0-9]+')\n",
    "    post_to_cv.job_title = [w for w in post_to_cv.job_title if not w in ['and', 'to', 'this', 'the']]\n",
    "    \n",
    "    ddata = dd.from_pandas(post_to_cv, npartitions=50)\n",
    "    post_to_cv['title_intersect'] = ddata.map_partitions(lambda df: df.apply((lambda row: intersection(row.position, row.job_title)), axis=1)).compute(scheduler='processes')\n",
    "    \n",
    "    jps_dic = post_to_cv[['job_profile_id','title_intersect']].set_index('job_profile_id').to_dict()['title_intersect']\n",
    "    learning_data['title_intersect'] = 0\n",
    "    idx = learning_data[learning_data.job_profile_id.isin(post_to_cv.job_profile_id.to_list())].index\n",
    "    learning_data.loc[idx, 'title_intersect'] = [jps_dic[w] for w in learning_data.loc[idx, 'job_profile_id']]\n",
    "    \n",
    "    return learning_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data_to_local():\n",
    "    Job_Post = pd.read_sql_query('''SELECT id,does_not_require_industry_experience,\n",
    "    does_not_require_practice_area_experience,sector_id,employment_type,job_title,role_id,\n",
    "    weekly_hours,asap_start,holidays,level,company_id,location_info,\n",
    "    maximum_salary,salary_unit,does_not_require_sector_experience,contract_type,\n",
    "    confidential_salary,postcode FROM customer_jobpost;''', cnx)\n",
    "    JobProfile = pd.read_sql_query('''SELECT id,candidate_id,job_post_id FROM  customer_JobProfile;''', cnx)\n",
    "    Companies = pd.read_sql_query('''SELECT id,size,name FROM company_company;''', cnx)\n",
    "    Candidates = pd.read_sql_query('''SELECT id,permanent_work,temp_to_perm_work,contract_work,\n",
    "    full_time_work,part_time_work,minimum_wage,postcode,commute_time,commute_by_car  FROM candidate_candidate;''', cnx)\n",
    "    CandidatesTalentPool = pd.read_sql_query('''SELECT candidate_id,seniority_qualifier,\n",
    "    _role_type_id,seniority FROM ops_CandidateTalentPool;''', cnx)\n",
    "    JobTalentPool = pd.read_sql_query('''SELECT job_id,_role_type_id,seniority FROM ops_JobTalentPool;''', cnx)\n",
    "\n",
    "    Job_Post = Job_Post.rename(columns={'id': 'job_post_id', 'does_not_require_sector_experience': 'req_sector_exp',\n",
    "                                        'does_not_require_practice_area_experience': 'req_practice_exp',\n",
    "                                        'does_not_require_industry_experience': 'req_industry_exp'})\n",
    "    JobProfile = JobProfile.rename(columns={'id': 'job_profile_id'})\n",
    "    Companies = Companies.rename(columns={'id': 'company_id', 'size': 'size_company'})\n",
    "    Candidates = Candidates.rename(columns={'id': 'candidate_id', 'postcode': 'postcode_c',\n",
    "                                            '_preferred_role_types': 'preferred_role_types'})\n",
    "    Job_Post['maximum_salary'] = Job_Post.apply(yearly_salary_to_unit, axis=1)\n",
    "    Job_Post = Job_Post.drop(['salary_unit'], axis=1)\n",
    "    return Job_Post, JobProfile, Companies, Candidates, CandidatesTalentPool, JobTalentPool\n",
    "\n",
    "\n",
    "def information_for_regression(data):\n",
    "    data[\"week_day\"] = [w.weekday() for w in data.created]\n",
    "    data[\"hour\"] = [w.hour for w in data.created]\n",
    "    data.created = data.created.dt.year * 12 + data.created.dt.month\n",
    "    data.created = data.created - min(data.created)\n",
    "    \n",
    "    data_collation = pd.merge(data, JobProfile, how=\"inner\", left_on=\"job_profile_id\",\n",
    "                              right_on=\"job_profile_id\").sort_values(['created'])\n",
    "    data_collation['seq_no'] = 1\n",
    "    data_collation['been_sent'] = data_collation['mean_of_invites_applied'] = data_collation['past_shortlist'], data_collation['invites_applied']ource = 0\n",
    "    for col in ['emp', 'con', 'com', 'role', 'title', 'sect', 'level', 'asap', 'confid', 'sal', 'applied']:\n",
    "        data_collation[\"similarity_\" + col] = 0\n",
    "    data_collation = pd.merge(data_collation, Job_Post, how=\"inner\", left_on=\"job_post_id\", right_on=\"job_post_id\")\n",
    "    data_collation = data_collation.groupby(['candidate_id']).apply(index_unable)\n",
    "    data_collation = pd.merge(data_collation, Candidates, how=\"inner\", left_on=\"candidate_id\", right_on=\"candidate_id\")\n",
    "    data_collation = pd.merge(data_collation, Companies, how=\"inner\", left_on=\"company_id\", right_on=\"company_id\")\n",
    "    data_collation = data_collation.dropna(subset=['minimum_wage'])\n",
    "    data_ = get_sourcing()  # Where each candidate was sourced from\n",
    "    data_.job_profile_id = pd.to_numeric(data_.job_profile_id, errors='coerce')\n",
    "    data_collation = pd.merge(data_collation, data_, how=\"inner\", left_on=\"job_profile_id\", right_on=\"job_profile_id\")\n",
    "    data_collation.set_index(pd.Index(range(len(data_collation))), inplace=True)\n",
    "\n",
    "    df_c = pd.concat([data_collation[\"postcode_c\"], data_collation[\"postcode\"]], axis=1)  # List of unique postcodes\n",
    "    for a in df_c:\n",
    "        a = a.lower()\n",
    "\n",
    "    results = []\n",
    "    unique_postcodes = []\n",
    "    for a in range(len(df_c)):\n",
    "        pc1 = df_c[\"postcode_c\"][a]\n",
    "        pc2 = df_c[\"postcode\"][a]\n",
    "        unique_postcodes.append(pc1)\n",
    "        unique_postcodes.append(pc2)\n",
    "        unique_postcodes = list(set(unique_postcodes))\n",
    "\n",
    "    for pcs in tqdm(list(grouper(90, unique_postcodes))):  # range((unique_postcodes.shape[0] // 100)+1)):\n",
    "        st = {\"postcodes\": pcs}\n",
    "        r = requests.post(url='http://api.postcodes.io/postcodes', data=st)  # converts to long and lat\n",
    "        results.extend(r.json()['result'])\n",
    "\n",
    "    postcodes_info = pd.DataFrame(results)\n",
    "    postcodes_info = postcodes_info.apply(lambda x: pd.Series(x['result']), axis=1)\n",
    "    postcodes_info = postcodes_info.dropna(subset=['postcode']).drop_duplicates(subset=['postcode'])\n",
    "    postcodes_info = postcodes_info.set_index(postcodes_info['postcode'].str.replace(' ', '').str.lower())\n",
    "\n",
    "    data_collation[\"distance\"] = df_c.apply(lambda y: distance_between_postcodes(y['postcode_c'], y['postcode'],\n",
    "                                                                                 postcodes_info), axis=1)\n",
    "    data_collation['region'] = data_collation.location_info.apply(lambda x: x.get(\"region\"))\n",
    "    data_collation.dropna(subset=['distance'], inplace=True)\n",
    "    data_collation['within_distance'] = data_collation.apply(lambda x: within_commute(x['distance'], x['region'],\n",
    "                                                                                      x['commute_time']), axis=1)\n",
    "    data_collation['intersect_seniority'] = data_collation.apply(intersect_seniority_, axis=1)\n",
    "    data_collation['intersect'] = data_collation.apply(intersect_, axis=1)\n",
    "\n",
    "    data_collation['contract_no_match'] = data_collation['employment_no_match'] = 0\n",
    "    data_collation = data_collation.apply(contract_employment, axis=1)\n",
    "    data_collation = data_collation[((np.abs(stats.zscore(data_collation['minimum_wage']))) < 3)]  # removes oddities\n",
    "\n",
    "    learning_data = data_collation.copy()\n",
    "    '''[['level','applied','candidate_id', 'job_post_id', 'intersect', 'intersect_seniority', 'seq_no', 'sector_id',\n",
    "    'mean_of_invites_applied', 'been_sent', 'source_i_1', 'source_i_2', 'source_i_3', \"job_profile_id\",\n",
    "    'source_e_1', 'source_e_2', 'source_e_3', 'source_e_4', 'source_e_5', 'source_e_6', 'source_e_7', 'source_e_8'\n",
    "    'source_e_10', 'source_e_11', 'distance', 'minimum_wage', 'contract_no_match', 'asap_start', 'size_company',\n",
    "    'holidays', 'employment_no_match', 'maximum_salary', 'confidential_salary', 'within_distance', 'source_e_9',\n",
    "    'req_industry_exp', 'req_sector_exp', \"similarity_applied\", 'req_practice_exp']]'''\n",
    "    learning_data.dropna(subset=['size_company', 'distance'], inplace=True)\n",
    "    learning_data.holidays.fillna(learning_data.holidays.mean(), inplace=True)\n",
    "    learning_data = learning_data.rename(columns={'level': 'post_seniority_level'})\n",
    "\n",
    "    Job_roles = list(JobTalentPool._role_type_id.value_counts().index)\n",
    "    Cand_roles = list(CandidatesTalentPool._role_type_id.value_counts().index)\n",
    "    role_matches = list(set(Job_roles) & set(Cand_roles))\n",
    "    columns_match = []\n",
    "    for role_type in role_matches:\n",
    "        learning_data[\"match_\" + str(role_type)] = 0\n",
    "        columns_match.append(\"match_\" + str(role_type))\n",
    "\n",
    "    learning_data = learning_data.apply(basically_tinder, axis=1)\n",
    "    for col in columns_match:\n",
    "        if learning_data[col].mean() < 0.001:\n",
    "            learning_data = learning_data.drop([col], axis=1)\n",
    "\n",
    "    learning_data = pd.get_dummies(learning_data, prefix=['size', 'sector'], columns=['size_company', 'sector_id'])\n",
    "\n",
    "    learning_data['confidential_salary'] = learning_data['confidential_salary'].astype(int)\n",
    "    learning_data['asap_start'] = learning_data['asap_start'].astype(int)\n",
    "    learning_data['req_sector_exp'] = learning_data['req_sector_exp'].astype(int)\n",
    "    learning_data['req_industry_exp'] = learning_data['req_industry_exp'].astype(int)\n",
    "    learning_data['req_practice_exp'] = learning_data['req_practice_exp'].astype(int)\n",
    "    learning_data = learning_data.set_index(pd.Index(range(len(learning_data))))\n",
    "\n",
    "    learning_data['within_income'] = 0\n",
    "    learning_data = learning_data.apply(within_salary, axis=1)\n",
    "    learning_data['minimum_wage'] = np.log(learning_data['minimum_wage'])\n",
    "    learning_data['maximum_salary'] = np.log(learning_data['maximum_salary'])\n",
    "\n",
    "    CandidateEs = pd.read_sql_query('''SELECT * FROM candidate_CandidateExperience;''', cnx)\n",
    "    cand_ids = learning_data.candidate_id.unique()\n",
    "    CandidateEs = CandidateEs[CandidateEs.candidate_id.isin(cand_ids)]\n",
    "    True_ongoing = CandidateEs[CandidateEs.ongoing is True].candidate_id.unique()\n",
    "    learning_data['is_last_job_ongoing'] = 0\n",
    "    cand_ids = learning_data[learning_data.candidate_id.isin(True_ongoing)].index\n",
    "    learning_data.loc[cand_ids, 'is_last_job_ongoing'] = 1\n",
    "    # learning_data = learning_data.apply(lambda x: True_on_going(x,True_ongoing), axis = 1) |replaces previous 2 lines\n",
    "\n",
    "    last_updated = CandidateEs.groupby('candidate_id')[['start_year', 'end_year']].max()\n",
    "    last_updated['years_since_cv_update'] = 2019 - last_updated[['start_year', 'end_year']].max(axis=1)\n",
    "    last_updated = last_updated.drop(['start_year', 'end_year'], axis=1)\n",
    "    last_updated.years_since_cv_update = last_updated.years_since_cv_update.fillna(2)  # CandidateEs.duration.dt.days\n",
    "    learning_data = pd.merge(learning_data, last_updated, how='outer', left_on='candidate_id', right_index=True)\n",
    "    learning_data = learning_data.sort_index()\n",
    "    learning_data.years_since_cv_update = learning_data.years_since_cv_update.fillna(2)  # arb\n",
    "\n",
    "    learning_data['sim_not_applied'] = learning_data['similarity_applied']  # splitting negative and positive\n",
    "    learning_data.sim_not_applied = learning_data.sim_not_applied.where(learning_data.sim_not_applied < 0, 0)\n",
    "    learning_data.similarity_applied = learning_data.similarity_applied.where(learning_data.similarity_applied > 0, 0)\n",
    "    learning_data['similarity_applied_sq'] = learning_data['similarity_applied']**2\n",
    "    learning_data['sim_not_applied_sq'] = learning_data['sim_not_applied']**2\n",
    "\n",
    "    learning_data = learning_data.drop(['candidate_id', 'job_post_id'], axis=1)\n",
    "    \n",
    "    learning_data = title_intersection(learning_data)  # similarity between last experience and job post description\n",
    "    learning_data = title_intersection(learning_data)  # Bag of words: experience position vs job title\n",
    "    \n",
    "    return learning_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = import_data(cnx)\n",
    "Job_Post, JobProfile, Companies, Candidates, CandidatesTalentPool, JobTalentPool = load_data_to_local()\n",
    "regression_data = information_for_regression(data)\n",
    "# regression_data.to_excel(\"regression_data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%pycodestyle\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, regression_data.applied, test_size=0.2)\n",
    "model = xgb.XGBClassifier(max_depth=10, scale_pos_weight=3.4)\n",
    "model = model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
